{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexo • All posts by \"hadoop\" category",
    "description": "",
    "home_page_url": "http://book.asmyun.com",
    "items": [
        {
            "id": "http://book.asmyun.com/2020/12/29/computer-science/hadoop/hadoop-install/",
            "url": "http://book.asmyun.com/2020/12/29/computer-science/hadoop/hadoop-install/",
            "title": "hadoop_install",
            "date_published": "2020-12-29T06:01:41.000Z",
            "content_html": "<ul>\n<li><a href=\"#hadoop%E4%BB%8B%E7%BB%8D\">hadoop 介绍</a></li>\n<li><a href=\"#%E8%AE%B0%E5%BD%95%E9%9B%86%E8%BF%9E%E6%8E%A5\">记录集连接</a></li>\n<li><a href=\"#%E8%AE%B0%E5%BD%95%E5%85%B3%E8%81%94\">记录关联</a></li>\n</ul>\n<ul>\n<li>\n<a href=\"#\">Post not found: kettle-install 安装kettle</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-one 编写第一个作业</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-control 组件介绍</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-demo kettle示例</a>\n</li>\n</ul>\n<h4 id=\"hadoop介绍\"><a class=\"markdownIt-Anchor\" href=\"#hadoop介绍\">#</a> hadoop 介绍</h4>\n<h4 id=\"hadoop下载\"><a class=\"markdownIt-Anchor\" href=\"#hadoop下载\">#</a> hadoop 下载</h4>\n<h4 id=\"hadoop安装\"><a class=\"markdownIt-Anchor\" href=\"#hadoop安装\">#</a> hadoop 安装</h4>\n<h4 id=\"hadoop配置\"><a class=\"markdownIt-Anchor\" href=\"#hadoop配置\">#</a> hadoop 配置</h4>\n<h4 id=\"hadoop异常解决\"><a class=\"markdownIt-Anchor\" href=\"#hadoop异常解决\">#</a> hadoop 异常解决</h4>\n<h5 id=\"error-but-there-is-no-hdfs_namenode_user-defined-aborting-operation\"><a class=\"markdownIt-Anchor\" href=\"#error-but-there-is-no-hdfs_namenode_user-defined-aborting-operation\">#</a> ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.</h5>\n<p>The root cause of this problem,</p>\n<p>hadoop install for different user and you start yarn service for different user. OR<br>\nin hadoop config’s <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcC1lbnYuc2g=\">hadoop-env.sh</span> specified HDFS_NAMENODE_USER and HDFS_DATANODE_USER user is something else.<br>\nHence we need to correct and make it consistent at every place. So a simple solution of this problem is to edit your <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcC1lbnYuc2g=\">hadoop-env.sh</span> file and add the user-name for which you want to start the yarn service. So go ahead and edit $HADOOP_HOME/etc/hadoop/hadoop-env.sh by adding the following lines</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HDFS_NAMENODE_USER=root</span><br><span class=\"line\">export HDFS_DATANODE_USER=root</span><br><span class=\"line\">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class=\"line\">export YARN_RESOURCEMANAGER_USER=root</span><br><span class=\"line\">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>\n<h5 id=\"error-java_home-is-not-set-and-could-not-be-found\"><a class=\"markdownIt-Anchor\" href=\"#error-java_home-is-not-set-and-could-not-be-found\">#</a> ERROR: JAVA_HOME is not set and could not be found.</h5>\n<p>没有配置好 java 环境变量<br>\n第一种临时修改环境变量<br>\n <code>export JAVA_HOME=/usr/java/jdk1.6.0_45</code> <br>\n 第二种修改 hadoop JAVA_HOME<br>\n <code>cd $HADOOP_HOME/etc/hadoop</code> <br>\n <code>vim hadoop-env.sh</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Many of the options here are built from the perspective that users</span></span><br><span class=\"line\"><span class=\"comment\"># may want to provide OVERWRITING values on the command line.</span></span><br><span class=\"line\"><span class=\"comment\"># For example:</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\">#  JAVA_HOME=/usr/java/testing hdfs dfs -ls</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># Therefore, the vast majority (BUT NOT ALL!) of these defaults</span></span><br><span class=\"line\"><span class=\"comment\"># are configured for substitution and not append.  If append</span></span><br><span class=\"line\"><span class=\"comment\"># is preferable, modify this file accordingly.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###</span></span><br><span class=\"line\"><span class=\"comment\"># Generic settings for HADOOP</span></span><br><span class=\"line\"><span class=\"comment\">###</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Technically, the only required environment variable is JAVA_HOME.</span></span><br><span class=\"line\"><span class=\"comment\"># All others are optional.  However, the defaults are probably not</span></span><br><span class=\"line\"><span class=\"comment\"># preferred.  Many sites configure these options outside of Hadoop,</span></span><br><span class=\"line\"><span class=\"comment\"># such as in /etc/profile.d</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The java implementation to use. By default, this environment</span></span><br><span class=\"line\"><span class=\"comment\"># variable is REQUIRED on ALL platforms except OS X!</span></span><br><span class=\"line\">export JAVA_HOME=/root/java/jdk1<span class=\"number\">.8</span><span class=\"number\">.0_271</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Location of Hadoop.  By default, Hadoop will attempt to determine</span></span><br><span class=\"line\"><span class=\"comment\"># this location based upon its execution path.</span></span><br><span class=\"line\"><span class=\"comment\"># export HADOOP_HOME=</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Location of Hadoop&#x27;s configuration information.  i.e., where this</span></span><br><span class=\"line\"><span class=\"comment\"># file is living. If this is not defined, Hadoop will attempt to</span></span><br><span class=\"line\"><span class=\"comment\"># locate it based upon its execution path.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># <span class=\"doctag\">NOTE:</span> It is recommend that this variable not be set here but in</span></span><br><span class=\"line\"><span class=\"comment\"># /etc/profile.d or equivalent.  Some options (such as</span></span><br><span class=\"line\"><span class=\"comment\"># --config) may react strangely otherwise.</span></span><br><span class=\"line\"><span class=\"comment\">#</span></span><br><span class=\"line\"><span class=\"comment\"># export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The maximum amount of heap to use (Java -Xmx).  If no unit</span></span><br><span class=\"line\"><span class=\"comment\"># is provided, it will be converted to MB.  Daemons will</span></span><br><span class=\"line\"><span class=\"comment\"># prefer any Xmx setting in their respective _OPT variable.</span></span><br><span class=\"line\"><span class=\"comment\"># There is no default; the JVM will autoscale based upon machine</span></span><br><span class=\"line\"><span class=\"comment\"># memory size.</span></span><br><span class=\"line\"><span class=\"comment\"># export HADOOP_HEAPSIZE_MAX=</span></span><br></pre></td></tr></table></figure>\n<h5 id=\"permission-denied-publickeygssapi-keyexgssapi-with-micpassword\"><a class=\"markdownIt-Anchor\" href=\"#permission-denied-publickeygssapi-keyexgssapi-with-micpassword\">#</a> Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).</h5>\n",
            "tags": []
        }
    ]
}